{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_datasets(control_path, cancer_path):\n",
    "    # Load datasets\n",
    "    control_df = pd.read_csv(control_path)\n",
    "    cancer_df = pd.read_csv(cancer_path)\n",
    "    \n",
    "    # Determine the size for undersampling\n",
    "    sample_size = len(cancer_df)\n",
    "    \n",
    "    # Undersample the healthy dataset\n",
    "    control_df_sample = control_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Combine datasets and shuffle\n",
    "    combined_df = pd.concat([control_df_sample, cancer_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split into features and labels\n",
    "    X = combined_df.drop(['cancer_type', 'type'], axis=1)\n",
    "    y = combined_df['cancer_type'].apply(lambda x: 1 if x != 'normal' else 0)  # 1 = cancer, 0 = healthy\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_data(X, n_components):\n",
    "    # Apply PCA (assumes X is already standardized)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, X_val, X_test, y_train, y_val, y_test, models):\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Save best parameters and performance\n",
    "        best_model = grid_search.best_estimator_\n",
    "        train_score = grid_search.best_score_\n",
    "        val_score = best_model.score(X_val, y_val)\n",
    "        test_score = best_model.score(X_test, y_test)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'train_score': train_score,\n",
    "            'val_score': val_score,\n",
    "            'test_score': test_score,\n",
    "            'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "models_to_test = {\n",
    "    'Random Forest': (RandomForestClassifier(random_state=42), {\n",
    "        'n_estimators': [50, 100],#, 200],\n",
    "        'max_depth': [None, 10],#, 20]\n",
    "    }),\n",
    "    'SVM': (SVC(random_state=42), {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }),\n",
    "    'Logistic Regression': (LogisticRegression(random_state=42, max_iter=500), {\n",
    "        'C': [0.1, 1, 10]\n",
    "    })\n",
    "}\n",
    "\n",
    "pca_to_test = [0, 10, 100, 160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "X, y = load_and_combine_datasets(\"Dataset/normal.csv\", \"Dataset/lung.csv\")\n",
    "cancer_type = \"lung\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running pca on 0 features\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training Logistic Regression...\n",
      "running pca on 10 features\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training Logistic Regression...\n",
      "running pca on 100 features\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training Logistic Regression...\n",
      "running pca on 160 features\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training Logistic Regression...\n",
      "Pipeline complete. Results saved to 'results.json'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "for num_pca in pca_to_test:\n",
    "    print(f\"running pca on {num_pca} features\")\n",
    "    # Run PCA feature reduction\n",
    "    if num_pca == 0:\n",
    "        X_train_pca = X_train_scaled\n",
    "        X_val_pca = X_val_scaled\n",
    "        X_test_pca = X_test_scaled\n",
    "    else:\n",
    "        X_train_pca, pca = pca_data(X_train_scaled, num_pca)\n",
    "        X_val_pca = pca.transform(X_val_scaled)\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    results = train_and_evaluate(X_train_pca, X_val_pca, X_test_pca, y_train, y_val, y_test, models_to_test)\n",
    "\n",
    "    # Save results\n",
    "    filename = f\"Result/results_{cancer_type}_pca_{num_pca}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    save_results(results, filename)\n",
    "\n",
    "print(\"Pipeline complete. Results saved to 'results.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
