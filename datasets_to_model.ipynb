{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_combine_datasets(control_path, cancer_path):\n",
    "    # Load datasets\n",
    "    control_df = pd.read_csv(control_path)\n",
    "    cancer_df = pd.read_csv(cancer_path)\n",
    "    \n",
    "    # Determine the size for undersampling\n",
    "    sample_size = len(cancer_df)\n",
    "    \n",
    "    # Undersample the healthy dataset\n",
    "    control_df_sample = control_df.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Combine datasets and shuffle\n",
    "    combined_df = pd.concat([control_df_sample, cancer_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Split into features and labels\n",
    "    X = combined_df.drop(['cancer_type', 'type'], axis=1)\n",
    "    y = combined_df['cancer_type'].apply(lambda x: 1 if x != 'normal' else 0)  # 1 = cancer, 0 = healthy\n",
    "    \n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_data(X, n_components):\n",
    "    # Apply PCA (assumes X is already standardized)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "def train_and_evaluate(X_train, X_val, X_test, y_train, y_val, y_test, models):\n",
    "    results = {}\n",
    "    \n",
    "    for model_name, (model, param_grid) in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', verbose=0, n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        train_score = grid_search.best_score_\n",
    "        val_score = best_model.score(X_val, y_val)\n",
    "        test_score = best_model.score(X_test, y_test)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        results[(model_name, X_train.shape[1])] = {\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'train_score': train_score,\n",
    "            'val_score': val_score,\n",
    "            'test_score': test_score,\n",
    "            'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results, output_file):\n",
    "    # Convert the results dictionary into a DataFrame\n",
    "    print(results)\n",
    "    rows = []\n",
    "    for model_name, result in results.items():\n",
    "        row = {\n",
    "            'model': model_name[0],\n",
    "            'pca_size': model_name[1],\n",
    "            'best_params': result['best_params'],\n",
    "            'train_score': result['train_score'],\n",
    "            'val_score': result['val_score'],\n",
    "            'test_score': result['test_score'],\n",
    "            'classification_report': str(result['classification_report'])  # Serialize the report as a string\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and parameter grids\n",
    "models_to_test = {\n",
    "    'Random Forest': (RandomForestClassifier(random_state=42), {\n",
    "        'n_estimators': [50, 100],#, 200],\n",
    "        'max_depth': [None, 10],#, 20]\n",
    "    }),\n",
    "    'SVM': (SVC(random_state=42), {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }),\n",
    "    'Logistic Regression': (LogisticRegression(random_state=42, max_iter=500), {\n",
    "        'C': [0.1, 1, 10]\n",
    "    })\n",
    "}\n",
    "\n",
    "pca_to_test = [0, 10, 100, 160]\n",
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "X, y = load_and_combine_datasets(\"Dataset/normal.csv\", \"Dataset/lung.csv\")\n",
    "cancer_type = \"lung\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running pca on 0 features\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training Logistic Regression...\n",
      "running pca on 10 features\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training Logistic Regression...\n",
      "running pca on 100 features\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training Logistic Regression...\n",
      "running pca on 160 features\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training Logistic Regression...\n",
      "{'Random Forest': {}, ('Random Forest', 54675): {'best_params': {'max_depth': None, 'n_estimators': 100}, 'train_score': 0.9813650128115537, 'val_score': 1.0, 'test_score': 1.0, 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 17.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}}}, 'SVM': {}, ('SVM', 54675): {'best_params': {'C': 0.1, 'kernel': 'linear'}, 'train_score': 0.9938271604938271, 'val_score': 0.9705882352941176, 'test_score': 1.0, 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 17.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}}}, 'Logistic Regression': {}, ('Logistic Regression', 54675): {'best_params': {'C': 0.1}, 'train_score': 0.9938271604938271, 'val_score': 0.9705882352941176, 'test_score': 1.0, 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 17.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}}}, ('Random Forest', 10): {'best_params': {'max_depth': None, 'n_estimators': 50}, 'train_score': 0.9442115071045888, 'val_score': 0.9705882352941176, 'test_score': 0.9428571428571428, 'classification_report': {'0': {'precision': 0.9, 'recall': 1.0, 'f1-score': 0.9473684210526316, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 0.8823529411764706, 'f1-score': 0.9375, 'support': 17.0}, 'accuracy': 0.9428571428571428, 'macro avg': {'precision': 0.95, 'recall': 0.9411764705882353, 'f1-score': 0.9424342105263158, 'support': 35.0}, 'weighted avg': {'precision': 0.9485714285714286, 'recall': 0.9428571428571428, 'f1-score': 0.9425751879699249, 'support': 35.0}}}, ('SVM', 10): {'best_params': {'C': 1, 'kernel': 'rbf'}, 'train_score': 0.9437456324248776, 'val_score': 0.9705882352941176, 'test_score': 0.9714285714285714, 'classification_report': {'0': {'precision': 1.0, 'recall': 0.9444444444444444, 'f1-score': 0.9714285714285714, 'support': 18.0}, '1': {'precision': 0.9444444444444444, 'recall': 1.0, 'f1-score': 0.9714285714285714, 'support': 17.0}, 'accuracy': 0.9714285714285714, 'macro avg': {'precision': 0.9722222222222222, 'recall': 0.9722222222222222, 'f1-score': 0.9714285714285714, 'support': 35.0}, 'weighted avg': {'precision': 0.9730158730158731, 'recall': 0.9714285714285714, 'f1-score': 0.9714285714285714, 'support': 35.0}}}, ('Logistic Regression', 10): {'best_params': {'C': 1}, 'train_score': 0.9379221989284883, 'val_score': 1.0, 'test_score': 0.9428571428571428, 'classification_report': {'0': {'precision': 0.9444444444444444, 'recall': 0.9444444444444444, 'f1-score': 0.9444444444444444, 'support': 18.0}, '1': {'precision': 0.9411764705882353, 'recall': 0.9411764705882353, 'f1-score': 0.9411764705882353, 'support': 17.0}, 'accuracy': 0.9428571428571428, 'macro avg': {'precision': 0.9428104575163399, 'recall': 0.9428104575163399, 'f1-score': 0.9428104575163399, 'support': 35.0}, 'weighted avg': {'precision': 0.9428571428571428, 'recall': 0.9428571428571428, 'f1-score': 0.9428571428571428, 'support': 35.0}}}, ('Random Forest', 100): {'best_params': {'max_depth': None, 'n_estimators': 100}, 'train_score': 0.9443279757745167, 'val_score': 0.9411764705882353, 'test_score': 0.9142857142857143, 'classification_report': {'0': {'precision': 0.8571428571428571, 'recall': 1.0, 'f1-score': 0.923076923076923, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 0.8235294117647058, 'f1-score': 0.9032258064516129, 'support': 17.0}, 'accuracy': 0.9142857142857143, 'macro avg': {'precision': 0.9285714285714286, 'recall': 0.9117647058823529, 'f1-score': 0.913151364764268, 'support': 35.0}, 'weighted avg': {'precision': 0.926530612244898, 'recall': 0.9142857142857143, 'f1-score': 0.9134349521446294, 'support': 35.0}}}, ('SVM', 100): {'best_params': {'C': 0.1, 'kernel': 'linear'}, 'train_score': 0.9753086419753086, 'val_score': 0.9705882352941176, 'test_score': 1.0, 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 17.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}}}, ('Logistic Regression', 100): {'best_params': {'C': 0.1}, 'train_score': 0.9876543209876543, 'val_score': 0.9705882352941176, 'test_score': 1.0, 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 17.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}}}, ('Random Forest', 160): {'best_params': {'max_depth': None, 'n_estimators': 100}, 'train_score': 0.9318658280922433, 'val_score': 0.9411764705882353, 'test_score': 0.9428571428571428, 'classification_report': {'0': {'precision': 0.9, 'recall': 1.0, 'f1-score': 0.9473684210526316, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 0.8823529411764706, 'f1-score': 0.9375, 'support': 17.0}, 'accuracy': 0.9428571428571428, 'macro avg': {'precision': 0.95, 'recall': 0.9411764705882353, 'f1-score': 0.9424342105263158, 'support': 35.0}, 'weighted avg': {'precision': 0.9485714285714286, 'recall': 0.9428571428571428, 'f1-score': 0.9425751879699249, 'support': 35.0}}}, ('SVM', 160): {'best_params': {'C': 0.1, 'kernel': 'linear'}, 'train_score': 0.9938271604938271, 'val_score': 0.9705882352941176, 'test_score': 1.0, 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 17.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}}}, ('Logistic Regression', 160): {'best_params': {'C': 0.1}, 'train_score': 0.9938271604938271, 'val_score': 0.9705882352941176, 'test_score': 1.0, 'classification_report': {'0': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 18.0}, '1': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 17.0}, 'accuracy': 1.0, 'macro avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}, 'weighted avg': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 35.0}}}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'best_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# save total resutls\u001b[39;00m\n\u001b[1;32m     31\u001b[0m csv_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult/results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcancer_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_pca_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m \u001b[43msave_results_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcsv_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline complete. Results saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36msave_results_to_csv\u001b[0;34m(results, output_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, result \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pca_size, performance \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m         row \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model_name,\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca_size\u001b[39m\u001b[38;5;124m'\u001b[39m: pca_size,\n\u001b[0;32m---> 10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_params\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mperformance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_params\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_score\u001b[39m\u001b[38;5;124m'\u001b[39m: performance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_score\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_score\u001b[39m\u001b[38;5;124m'\u001b[39m: performance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_score\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m'\u001b[39m: performance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(performance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification_report\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Serialize the report as a string\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         }\n\u001b[1;32m     16\u001b[0m         rows\u001b[38;5;241m.\u001b[39mappend(row)\n\u001b[1;32m     18\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'best_params'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Preprocess data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "for num_pca in pca_to_test:\n",
    "    print(f\"running pca on {num_pca} features\")\n",
    "    # Run PCA feature reduction\n",
    "    if num_pca == 0:\n",
    "        X_train_pca = X_train_scaled\n",
    "        X_val_pca = X_val_scaled\n",
    "        X_test_pca = X_test_scaled\n",
    "    else:\n",
    "        X_train_pca, pca = pca_data(X_train_scaled, num_pca)\n",
    "        X_val_pca = pca.transform(X_val_scaled)\n",
    "        X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    model_results = train_and_evaluate(X_train_pca, X_val_pca, X_test_pca, y_train, y_val, y_test, models_to_test)\n",
    "\n",
    "    # Update results\n",
    "    all_results.update(model_results)\n",
    "    \n",
    "# save total resutls\n",
    "csv_filename = f\"Result/results_{cancer_type}_pca_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "save_results_to_csv(results=all_results, output_file=csv_filename)\n",
    "\n",
    "print(\"Pipeline complete. Results saved to 'results.json'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
